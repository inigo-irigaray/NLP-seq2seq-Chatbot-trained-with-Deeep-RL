# Leveraging the power of Deep Reinforcement Learning training NLP algorithms


<p align=justify><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This project empirically shows the benefits of combining Deep Reinforcement Learning (DLR) methods with popular Natural Language Processing (NLP) algorithms in the pursuit of state-of-the-art results in dialogue systems and other human language comprehension tasks. The experiment is based on the simple Cornell University Movie Dialogs database and integrates the sequence-to-sequence (seq2seq) model of LSTM networks into cross-entropy learning for pretraining and into the REINFORCE method. Thus, the algorithm leverages the power of stochasticity  inherent to Policy Gradient (PG) models and directly optimizes the BLEU score, while avoiding getting the agent stuck through transfer learning of log-likelihood training. This combination results in improved quality and generalization of NLP models and opens the way for stronger algorithms for various tasks, including those outside the human language domain.</b></p>

-------
<p align=justify><b><a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL#1-preliminaries>1. Preliminaries.</a></b> Introduces a conceptual background on the NLP literature and state-of-the-art algorithms for conversational modelling, machine translation and other key challenges in the field.</p>

<b><a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL#2-seq2seq-with-cross-entropy--reinforce>2. seq2seq with Cross-Entropy & REINFORCE - the algorithms.</a></b> Details the specifics of the algorithms used for this particular experiment and the core structure of the approximation models employed.</p>

<b><a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL#3-training--results>3. Training & Tests Discussion.</a></b> Analyzes the progress of the two different training methods until halting, and the corresponding performance of the model on the tests.</p>

<b><a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL#4-future-work>4. Future work.</a></b> Explores potential avenues of interest for future experiments.</p>


---------
## 1. Preliminaries

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The recent advancements on deep neural network architectures, sustained on improved computational capabilities and larger, more comprehensive datasets, have propelled a vast amount of success in the field of Machine Learning. This, coupled with better systems for vectorial representation of language structures in the form of embeddings, has put Natural Language Processing at the forefront of research and progress. The following subsections serve as an overview of major methods for different NLP tasks and the works that led to this implementation of seq2seq based on Ranzato et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#9-m-ranzato-s-chopra-m-auli-and-w-zaremba-sequence-level-training-with-recurrent-neural-networks-2015>9</a>].</p>

#### Embeddings

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Embeddings are distributional vectors representing different levels of linguistic structures (characters and words). They capture meaning by encoding reference attributes to each structure based on the context in which it appears, i.e. the other words and characters that tend to be next to the target structure [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#6-d-jurafsky-and-jh-martin-speech-and-language-processing-unpublished-draft-2019>6-Ch6</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Word2vec</b> are some of the most popular word embedding algorithms. The <b>skip-gram</b> model predicts context words based on the target word. It trains a logistic regression classifier that computes the conditional probability between pairs of words with the dot-product between their embeddings. Opposite to skip-gram, the continuous bag-of-words (<b>CBOW</b>) predicts a target word from the context words. Based on fully-connected NNs with one hidden-layer, these methods allow for efficient representations of dense vectors that capture semantic and syntactic information. However, they are weak on sentiment, polisemy, phrase meaning, conceptual meaning and out-of-vocabulary (OOV) words, which is problematic for some tasks [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#6-d-jurafsky-and-jh-martin-speech-and-language-processing-unpublished-draft-2019>6-Ch6</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Contextualized word embeddings are another type of embeddings that directly address some of these issues. <b>ELMo</b> creates a different word embedding for each context in which a word appears, thus capturing polisemic meaning. It consists of a bidirectional language model with a forward Long Short-Term Memory (LSTM) network that calculates the joint probability of a series of input tokens and predicts the next token, a backward LSTM network that predicts the previous token and the cross-entropy loss between the two predictions.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alternatively, approaching the embedding problem at the character-level has allowed researchers to tackle some issues aforementioned and tasks like named-equity recognition (NER), adding meaning to phrases by representing words simply as a combination of characters. Additionally, they prove more effective with some morphologically-rich languages like Spanish, and languages where text is composed of individual characters instead of separated words like Chinese. Some of these algorithms include character trigrams and skip-grams as bag-of-character n-grams [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#16-t-young-d-hazarika-s-poria-and-e-cambria-recent-trends-in-deep-learning-based-natural-language-processing-2017>16</a>].</p>

#### RNNs

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The most popular neural network architectures are recurrent neural networks (<b>RNNs</b>), since they are able to capture the sequential nature of language through the transfer of a latent state called the hidden state to the next input in the network. Thus, each input becomes dependent on the sequence of previous inputs, in addition to being dependent on itself.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The most basic model is the <b>vanilla RNN</b>. The network takes as input the hidden state from the previous input state, <i>h-1</i> and the current input state, <i>x</i>. To calculate the current hidden state, it first multiplies <i>h-1</i> and <i>x</i> by two weight matrices and then applies a non-linearity to the sum of both. The current hidden state is passed onto the next RNN in the process and the output of the network is subsequently calculated as a non-linearity of the current hidden state times a weight matrix. This individual RNN processes can be put together following different model architectures, such as taking one RNN's process output as the next RNN state input <i>x</i>, and many more.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Vanilla RNNs suffer from the exploding/vanishing gradients problem. <b>LSTM</b> networks were specifically designed to address this issue. They consist of forget, input and output gates. LSTM calculates the cell state and the hidden state as functions of these gates. The three different gates are calculated as a sigmoid of weights matrices times the concatenated <i>h-1 & x</i> plus a bias. The forget gate decides what to forget from the previous cell state, the input gate decides which inputs will be updated and the output gate helps to calculate the next hidden state. Finally, the current cell state is calculated as the dot-product between the forget gate and the previous cell state plus the dot-product between the input gate and a non-linearity of a weight matrix times the concatenated <i>h-1 & x</i> plus a bias. The latter terms regulates the network. Finally, the next hidden state is caculated as a dot-product of the output gate and a non-linearity of the current cell state.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gated recurrent units (<b>GRUs</b>) are another proposal which follows the same philosophy as LSTMs but are computationally cheaper and faster, since they perform fewer tensor operations. It gets rid of the cell state memory and includes two gates: the update gate and reset gate. The reset gate decides how much past information to forget and the update gate how much of the previous hidden state to do away with and how much of the current network state to add. Both LSTMs and GRUs have proven to be superior in quality to vanilla RNNs, but do not show huge differences in outcome among them.</p>

#### CNNs

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Convolutional neural networks (<b>CNNs</b>) are very effective feature abstraction tools that can extract high-level information from large corpora and their embedded representations. CNNs have been used to create latent semantic representations of sentences, obtaining a global summarization of the sentence features through deep layers of convolutions [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#16-t-young-d-hazarika-s-poria-and-e-cambria-recent-trends-in-deep-learning-based-natural-language-processing-2017>16</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Often times, however, word-level representations are required for many NLP tasks and RNNs have traditionally been prioritized since they are designed to capture the sequential nature of language while CNNs draw a broader generalized overall picture. Some recent works have been able to address sequence modelling through the <b>window approach</b>. Convolutions are applied to a window of words of size <i>k</i> around the target word. Thus, the CNN is able to extract contextual meaning for words from its neighbors.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Especially relevant has been the success of Gated CNNs (<b>GCNNs</b>), which have beaten previous state-of-the-art results from LSTM-based recurrent models at some NLP tasks. This architecture consists of encoder-decoder convolution models and is constructed upon 4 solid foundations. Firstly, it employs gated linear units (<b>GLUs</b>) non-linearities that allow the networks to change the scope of abstraction from the full input field to fewer elements within it by covenience. Secondly, for decoder networks it <b>caps the convolution window</b> at the front so it will not learn to make word predictions having already considered future information. Thirdly, it uses <b>residual connections</b> from the input to layer outputs as proposed in <i>ResNet</i> that allow for deeper convolutions. Finally, it employs a <b>multi-step attention mechanism</b> which informs the decoder about the full history of previous inputs having been considered, while RNNs may partially lose this sequence information as it travels through multiple non-linearities [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#1-yn-dauphin-a-fan-m-auli-and-d-grangier-language-modeling-with-gated-convolutional-networks-2016>1</a>, <a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#3-j-gehring-m-auli-d-grangier-d-yarats-and-yn-dauphin-convolutional-sequence-to-sequence-learning-2017>3</a>].</p>

#### Recursive NNs

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential processing of sentences has yielded some positive results. However, it sometimes fails to capture the compositionality aspect of languages. Longer sentences can underperform in previous models, especially in tasks like sentiment analysis, because they do not take into account that words group into phrases before reaching the higher structural level of sentences.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recursive neural networks are based on constituency parsing trees and are better at this job. One of the most succesful architectures so far has been the recurrent neural tensor network (<b>RNTN</b>). Words are represented as vectors and leaf nodes within the tree structure. The model performs matrix and tensor computations to move from the leaf nodes upwards, as can be seen in Figure 1. Here, the <i>f</i> function represents a non-linearity and the indexes of the <i>V</i> tensor the dimensionality of the tensor. When the model reaches the root node, it performs a matrix multiplication with a set of weights and finally applies a softmax activation function. Thus, it is able to capture sentence sentiment with extraordinarily high  levels of accuracy [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#12-r-socher-a-perelygin-jy-wu-j-chuang-cd-manning-ay-ng-and-c-potts-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank-2013>12</a>].</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/RNTNtree.png" height=255 width=446><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/RNTN.png" height=255 width=300></p>
<p align="center"><b>Figure 1:</b> RNTN. <b>1 left</b> RNTN tree. <b>1 right</b> RNTN matrix operations. <b>Source:</b>Own elaboration and Socher et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#13-s-sukhbaatar-a-szlam-j-weston-and-r-fergus-end-to-end-memory-networks-2015>13</a>].</p>

#### Generative

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DeepFakes have put deep generative models at the center of controversy and discussion, especially with fake videos about high-ranking officials. There has been some research in the last couple of years seeking to apply these models in NLP tasks, revolving around the two most common structures: VAEs and GANs.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Variational autoencoders (<b>VAEs</b>) consist of an encoder-decoder scheme of NNs whichs is iteratively optimized. Unlike vanilla autoencoders, VAEs regularize the latent space according to a Gaussian normal distribution in order to obtain a stable and reliable representation of language structures so that the generator can sample new meaningful states from it. The loss function is calculated as the sum of the reconstruction error loss (the mean-squared error between the encoded input and decoded output) and the regularisation loss (the Kullback-Leibler divergence between the encoded distribution and the normal Gaussian distribution) [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#10-j-rocca-understanding-variational-autoencoders-vaes-towards-data-science-2019>10</a>]. Some modifications have successfully been implemented, like augmenting the unstructured <i>z</i> space with structured <i>c</i> variables which target specific salient semantic features of sentences (e.g. a positive/negative attribute for sentiment) [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#5-z-hu-z-yang-x-liang-r-salakhutdinov-and-ep-xing-toward-controlled-generation-of-text-arxiv170300955-2017>5</a>].</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/VAE.png" height=305 width=839></p>
<p align="center"><b>Figure 2:</b> VAE generation diagram. <b>Source:</b> Rocca [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#10-j-rocca-understanding-variational-autoencoders-vaes-towards-data-science-2019>10</a>].</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/VAEs.png" height=305 width=645></p>
<p align="center"><b>Figure 3:</b> VAE loss and NNs diagram. <b>Source:</b> Rocca [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#10-j-rocca-understanding-variational-autoencoders-vaes-towards-data-science-2019>10</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generative adversarial networks (<b>GANs</b>) are systems consisting of a neural network generator and discrimator. The generator will be fed random noise, from which it will create new data which replicates as much as possible the underlying distribution of some real data. The job of the generator is to trick the discriminator into believing that its output comes from real data (discriminator accuracy of 0.5), while the discriminator's mission is to separate fake from real inputs [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#4-ij-goodfellow-j-pouget-abadie-m-mirza-b-xu-d-warde-farley-s-ozair-a-courville-and-y-bengio-generative-adversarial-networks-arxiv14062661-2014>4</a>]. In 2017 a GAN adaptation by Yang et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#15-z-yang-w-chen-f-wang-and-b-xu-improving-neural-machine-translation-with-conditional-sequence-generative-adversarial-nets-arxiv170304887-2017>15</a>] achieved state-of-the-art results on machine translation by constructing a GAN with a transformer generator, a CNN discriminator and BLEU reinforced learning called BR-CSGAN.</p>

#### Attention Mechanisms

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In encoder-decoder systems sometimes encoders are inefficiently forced to encode embeddings that are not fully relevant. Attention mechanisms bound decoders by a history of the input data in addition to the previous latent state and generated token. This works as a mapping between certain value pairs and allows the network to focus on specific data from the whole dataset, essentially adding context at different decoding timesteps [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#6-d-jurafsky-and-jh-martin-speech-and-language-processing-unpublished-draft-2019>6-Ch10</a>, <a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#16-t-young-d-hazarika-s-poria-and-e-cambria-recent-trends-in-deep-learning-based-natural-language-processing-2017>16</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;End-to-end memory networks (<b>MemNets</b>) adopt this approach in a way that the attention system resembles a sort of internal memory. The model stores all the embedded input sentences in a memory and embeds the query as well. Importance weights of the memory input items are calculated by taking a softmax of the dot-product between the embedded query and the input memory. These weights represent the attention or importance given to each input data. The importance-adjusted input data is then added to the embedded query, and processed through a weight matrix and a softmax to generate the final prediction. This process can be performed in an iterative way as shown in Figure 4 for more clarity [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#13-s-sukhbaatar-a-szlam-j-weston-and-r-fergus-end-to-end-memory-networks-2015>13</a>].</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/MemNet.png" height=305 width=675></p>
<p align="center"><b>Figure 4:</b> MemNet. <b>4a</b> Single-layer MemNet. <b>4b</b> Multi-layer MemNet. <b>Source:</b> Sukhbaatar et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#12-r-socher-a-perelygin-jy-wu-j-chuang-cd-manning-ay-ng-and-c-potts-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank-2013>12</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another landmark architecture employing attention mechanisms is the <b>transformer</b> network. This model replaces the RNNs and CNNs typically used in encoder-decoder frameworks with attention layers, as shown in Figure 5. The encoder consists of a series of identical stacked layers, each with two sublayers: a <b>multi-head</b> attention mechanism and a normal fully-connected network. The decoder is similar to the encoder, but it includes an extra multi-head attention layer to process encoder output. Additionally, information from future positions in the input to the first multi-head layer is masked, since it would be cheating to make predictions with input from steps ahead. The model uses residual connections in each sublayer, followed by layer normalization.</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/transformer.png" height=450 width=335></p>
<p align="center"><b>Figure 5:</b> Transformer network. <b>Source:</b> Vaswani et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#14-a-vaswani-n-shazeer-n-parmar-j-uszkoreit-l-jones-an-gomez-and-l-kaiser-attention-is-all-you-need-2017>14</a>].</p>


<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Multi-head attention layers perform parallel dot-product attention functions on the queries, keys and values, which have been previously linearly projected <i>h</i> times. The outputs are then concatenated and projected linearly again. Thus, the model is able to learn from different representation subspaces at different positions [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#14-a-vaswani-n-shazeer-n-parmar-j-uszkoreit-l-jones-an-gomez-and-l-kaiser-attention-is-all-you-need-2017>14</a>].</p>

<p align="center"><img src="https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/images/multihead.png" height=305 width=675></p>
<p align="center"><b>Figure 6:</b> Multi-head attention function. <b>6 left</b> Dot-product attention. <b>6 right</b> Multi-head attention. <b>Source:</b> Vaswani et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#14-a-vaswani-n-shazeer-n-parmar-j-uszkoreit-l-jones-an-gomez-and-l-kaiser-attention-is-all-you-need-2017>14</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Building upon the <i>transformer</i> model, Open AI released the generative pre-training model (<b>GPT</b>). The training process consisted of 2 stages, unsupervised pretraining of the transformer algorithm, followed by a supervised fine-tuning, task-specific process. The former trains the transformer for language modelling on a corpus of unsupervised tokens by maximizing the log loss of conditional probabilities of the current token and a context window of past token sequences. The latter assumes a labelled set of data to pass through the transformer architecture, after which a linear transformation layer is added with a softmax activation function for prediction. It then maximizes the log loss between predicted and actual inputs plus a language modelling loss similar to the one used for unsupervised training [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#8-a-radford-k-narasimhan-t-salimans-and-i-sutskever-improving-language-understanding-by-generative-pre-training--2017>8</a>].</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>BERT</b>

#### Deep reinforcement learning applications

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With the advancement of deep reinforcement learning some new training techniques have been proposed, where the generator networks are considered as the agent and the objective metric as the reward to be maximized [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#16-t-young-d-hazarika-s-poria-and-e-cambria-recent-trends-in-deep-learning-based-natural-language-processing-2017>16</a>]. One of these methods is the one proposed by Ranzato et al. [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#9-m-ranzato-s-chopra-m-auli-and-w-zaremba-sequence-level-training-with-recurrent-neural-networks-2015>9</a>] which is the basis for this experiment and will be explained in depth in the next section.</p>

## 2. Seq2seq with Cross-Entropy & REINFORCE

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This section explains in detail the different algorithms employed in this experiment to solve a dialog system task individually, as well as the combination of them based on Ranzato et al.'s [<a href=https://github.com/inigo-irigaray/NLP-seq2seq-with-DeepRL/blob/master/README.md#9-m-ranzato-s-chopra-m-auli-and-w-zaremba-sequence-level-training-with-recurrent-neural-networks-2015>9</a>] work that demonstrate the benefits of deep reinforcement learning for NLP.</p>
  
#### seq2seq

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The most basic necessary structure for any dialog system task, as for machine translation and other similar NLP tasks, is an algorithm capable of understanding a message to answer accordingly. This can be done at the word-level or at a higher level: sequences of words.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The sequence-to-sequence (<b>seq2seq</b>) algorithm is a black box model that captures an input sequence of letters or words and generates a response output sequence. The heart of that black box is a generic encoder-decoder structure, where the hidden state of the encoder models the context of the input sequence, based on which the decoder will generate its corresponding output. The architecture of the model is similar to others previously introduced in Section 1. It consists of an initial embedding layer that stores word embeddings of the dictionary in a look up table of dimension 50, wheres indices of the words in the dictionary refer to their corresponding vectorized representation. Additionally, the encoder and decoder are unidirectional, single-layer LSTM networks with 512 features in the hidden state, tensors structured as (batch, seq, feature) (batch_first mode <i>True</i>) and no dropout. Finally, the decoder output passes through a linear transformation in a projection layer that converts it into a probability distribution of the dictionary tokens.</p>

#### Cross-Entropy (log-likelihood)

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The traditional approach to train algorithms for dialog systems has been to treat the task as a simple classification problem, using the <b>cross-entropy method</b>. Cross-entropy trains text generation models by optimizing predictions at the word level. The most natural form of this method is to feed tokens from the reference sentence into the encoder. Then, calculate the log loss between the output generated by the decoder and the next token in the target sequence. This loss will be optimized using stochastic gradient descent (SGD) and backpropagated through the decoder and encoder neural networks for making better future predictions. This approach to cross-entropy training is called <b>teacher-forcing</b> (since the correct outputs are being force-fed into the algorithm).</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This training mechanism allows the model to train fast solely on the real data distribution, which includes information from future tokens. However, at test time or real-life situations, the model will not have access to the real data sequences and will only perceive the underlying distribution that represents the past tokens in the sequence, with no future data to rely on. This inconsistency between the environment in which the model is trained and the environment in which the model is released to solve the problem at hand is called <i>exposure bias</i>. And it often implies that in real-life, if the model makes minor mistakes in early-stage predictions, it will fail miserably to solve the challenge.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An alternative approach that addresses this issue is called <b>curriculum learning</b>. This method trains the model on the distribution of its own past predictions, instead of being fed the distribution of the full target sequence to correct its own predictions. This is just like real-life situations and will help the seq2seq algorithm to model the language better for generic dialog systems. However, it increases training time dramatically, which makes it unfeasible in many situations.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This experiment follows a combined approach of the two previous cross-entropy methodologies called Data as Demonstrated (<b>DAD</b>). DAD alternates randomly at each time step between feeding the target sequence into the decoder (teacher-forcing) and feeding the argmax of previous prediction underlying distributions (curriculum learning). Thus, it can train the model at a reasonable speed, while doing a better job generalizing the model for the dialog generation problem.</p> 

#### REINFORCE

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another way to tackle NLP tasks is to frame them as reinforcement learning problems. The DRL methos used in this experiment is the <b>REINFORCE</b> algorithm. The environment in this task is the words and context vector the agent, the generator (the LSTM seq2seq), interacts with (the ones it takes as input). The agent parameterizes a policy according to which it will produce the next word in the sequence, i.e. according to which it will take an action. This will result in the agent updating its internal state, the hidden state. Once the end of the episode is reached (the end of a sequence), the agent gets a reward in the form of a performance metric (BLEU in this experiment, which will be explained in the next section). Since the BLEU metric is not differentiable, instead of performing backpropagation, REINFORCE will just push up the probabilities of successful episodes and decrease the others.</p>

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This approach has multiple benefits. Firstly, the stochasticity inherent to REINFORCE automatically accounts for the multiple possible target sequences. Since there could many answeers to a question as simple as "How are you?". Cross-entropy learns an oaverage of those replies, which may be suboptimal. However, REINFORCE learns the variant answers it could produce sampling from the probability distribution. Secondly, it optimizes directly the objective metric BLEU, which is our goal. Thirdly, it completely avoids exposure bias.</p>
  
<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;However, it also presents some drawbacks. REINFORCE is an example of Monte-Carlo learning methods and, like the rest of them, it suffers from high variance of the gradients. One common way of tackling this issue is substracting a baseline value from the <i>Q-values</i> in the loss function to add stability to the model. In the popular Actor-Critic (A2C) algorithm, the state value is used as a baseline. However, this experiment uses a different baseline, the deterministic policy calculated by setting the decoder in <i>argmax</i> mode instead of random sampling. A second problem arising from REINFORCE is that it is unfeasible to train the model from scratch. Phrase sequences have multiple words, at least five or more, and dictionary dimensions are usually in the thousands. Initializing weights for the LSTM networks meand that the probability of reaching a successful outcome in the early stages of training is infinitely small. A clever way to address the problem proposed by Ranzato et al. and followed in this experiment is to begin training with the cross-entropy method, switching randomly between teacher-forcing and curriculum learning, then transfer the learned weights into the REINFORCE method and initialize the network with them to guide to the model over the narrower search area for improvement.</p>

## 3. Training & Results

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For this experiment we consider the results of two training methods: simple cross-entropy learning and cross-entropy with deep reinforcment learning. To measure performance of the model, training progress and comparison purposes, the BLEU metric is used to measure the accuracy of the outputs generated.</p>

#### Bilingual evaluation uderstudy (BLEU)

<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The bilingual evaluation understady score (<b>BLEU</b>) is a standard metric used to compare input and output sequences of NLP models. BLEU calculates the geometric mean over n-grams (4 in this experiment) of overlapping words and the brevity penalty, i.e. it calculates the ratio of shared 4-grams (in our experiment) shared bewteen output and input and penalizes divergence between length of outputs and the shortest reference sentence. This metric is useful for some NLP tasks in the English language, but it is not as good with morphologically rich languages, capturing precise meaning and does not always guarantee that an answer will sound human-like, although grammarly correct. However, it does offer a benchmark quantitative measure which helps a lot to track progress for research on the NLP field.</p>
  
#### Cornell University Movie Dialogs Corpus  
  
<p align=justify>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The model in this experiment is trained on the Cornell University Movie Dialogs corpus, specifically the Comedy genre. The dataset at large consists of short conversations extracted from movie scripts, annoted with rich metadata including genre, year, rating etc. It includes 617 movies, 220,000 dialogues, 300,000 utterances, 150,000 phrase pairs and a dictionary size of 11,131 words. The comedy genre on which the experiment is trained includes only 159 movies, 22,000 phrase pairs and a dictionary size of 4,905. Dialog systems require very rich and large datasets to produce good results in BLEU scores and human-like answers. However, for the experiment we used this limited corpus, and even shorter subset (comedy) as training data due to personal resource limits that would not allow for training on larger datasets (processing capabilities, cost etc.).</p>
  
#### Training & Tests

general description and analysis

27% improvement. results deteriorating, while training improving -> overfitting to the limited dataset base of dialogues

## 4. Future Work


## References

#### <p>[1] Y.N. Dauphin, A. Fan, M. Auli and D. Grangier, "Language Modeling with Gated Convolutional Networks", 2016.</p>

#### <p>[2] J. Devlin, M. Chang, K. Lee and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", <i></i>, 2018.</p>

#### <p>[3] J. Gehring, M. Auli, D. Grangier, D. Yarats and Y.N. Dauphin, "Convolutional Sequence to Sequence Learning", 2017.</p>

#### <p>[4] I.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio, "Generative Adversarial Networks", <i>arXiv:1406.2661</i>, 2014.</p>

#### <p>[5] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov and E.P. Xing, "Toward Controlled Generation of Text", <i>arXiv:1703.00955</i>, 2017.</p>

#### <p>[6] D. Jurafsky and J.H. Martin, "Speech and Language Processing (Unpublished Draft)", 2019.</p>

#### <p>[7] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M, Lewis, L. Zettlemoyer and V. Stoyanov, "RoBERTa: A Robustly Optimized BERT Pretraining Approach", <i></i>, 2019.</p>

#### <p>[8] A. Radford, K. Narasimhan, T. Salimans and I. Sutskever, "Improving Language Understanding by Generative Pre-Training", <i></i>, 2017.</p>

#### <p>[9] M. Ranzato, S. Chopra, M. Auli and W. Zaremba, "Sequence Level Training with Recurrent Neural Networks", 2015.</p>

#### <p>[10] J. Rocca, "Understanding Variational Autoencoders (VAEs)", <i>Towards Data Science</i>, 2019.</p>

#### <p>[11] V. Sanh, L. Debut, J. Chaumond and T. Wolf, "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", <i></i>, 2019.</p>

#### <p>[12] R. Socher, A. Perelygin, J.Y. Wu, J. Chuang, C.D. Manning, A.Y. Ng and C. Potts, "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", 2013.</p>

#### <p>[13] S. Sukhbaatar, A. Szlam, J. Weston and R. Fergus, "End-to-end Memory Networks", 2015.</p>

#### <p>[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez and L. Kaiser, "Attention is All You Need", 2017.</p>

#### <p>[15] Z. Yang, W. Chen, F. Wang and B. Xu, "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets", <i>arXiv:1703.04887</i>, 2017.</p>

#### <p>[16] T. Young, D. Hazarika, S. Poria and E. Cambria, "Recent Trends in Deep Learning Based Natural Language Processing", 2017.</p>
