{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xentr_train",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNWGyHRhCHFP",
        "colab_type": "code",
        "outputId": "11291fa6-38eb-4685-c1ed-496c31d4409d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gSNk820qa8Q",
        "colab_type": "code",
        "outputId": "599e2d98-fe50-4f1d-dd1d-874d456ba8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-11 12:58:10--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.224.175.112, 34.206.134.194, 3.229.196.117, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.224.175.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  18.9MB/s    in 0.7s    \n",
            "\n",
            "2019-12-11 12:58:11 (18.9 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUR53xTM8HDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import data\n",
        "import model\n",
        "import utils\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLa6FwzHrQ1C",
        "colab_type": "code",
        "outputId": "0af7b49b-998d-496d-8fa1-0bb1414930c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "LOG_DIR = 'runs'\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://e55158fe.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTXLgAhlCWS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVES_DIR = \"saves\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_EPOCHS = 100\n",
        "\n",
        "log = logging.getLogger(\"train\")\n",
        "\n",
        "TEACHER_PROB = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qB8alraCprM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_test(test_data, net, end_token, device='cpu'):\n",
        "  \"\"\"Calculates the mean BLEU score for the held-out test dataset, which is 5% of laoded data by default.\"\"\"\n",
        "  bleu_sum = 0.0\n",
        "  bleu_count = 0\n",
        "  for p1, p2 in test_data:\n",
        "    input_seq = model.pack_input(p1, net.emb, device)\n",
        "    enc = net.encode(input_seq)\n",
        "    _, tokens = net.decode_chain_argmax(enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS, stop_at_token=end_token)\n",
        "    bleu_sum += utils.calc_bleu(tokens, p2[1:])\n",
        "    bleu_count += 1\n",
        "  return bleu_sum / bleu_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6EQGLVhbqCQg",
        "outputId": "00a8ccbb-c0c7-4891-9848-13201d31a96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "  \n",
        "  name = \"inigo\"\n",
        "  genre = \"comedy\" #specifies the film genre we want to train on\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  saves_path = os.path.join(SAVES_DIR, name)\n",
        "  os.makedirs(saves_path, exist_ok=True)\n",
        "  #load the dataset, save the embeddings dictionary(a mapping from the token's string to the integer ID of the token) and encode the phrase pairs.\n",
        "  phrase_pairs, emb_dict = data.load_data(genre_filter=genre)\n",
        "  log.info(\"Obtained %d phrase pairs with %d uniq words\", len(phrase_pairs), len(emb_dict))\n",
        "  data.save_emb_dict(saves_path, emb_dict)\n",
        "  end_token = emb_dict[data.END_TOKEN]\n",
        "  train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "  #split data into tarin/test parts, shuffle data using a fixed random seed(to be able to repeat the same shuffle at RL training stage).\n",
        "  rand = np.random.RandomState(data.SHUFFLE_SEED)\n",
        "  rand.shuffle(train_data)\n",
        "  log.info(\"Training data converted, got %d samples\", len(train_data))\n",
        "  train_data, test_data = data.split_train_test(train_data)\n",
        "  log.info(\"Train set has %d phrases, test %d\", len(train_data), len(test_data))\n",
        "  #create the model\n",
        "  net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE).to(device)\n",
        "  log.info(\"Model: %s\", net)\n",
        "  writer = SummaryWriter(comment=\"-\" + name)\n",
        "  optimiser = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "  best_bleu = None\n",
        "\n",
        "  for epoch in range(MAX_EPOCHS):#each epoch is an iteration over the batches of pairs of the encoded phrases\n",
        "    losses = []\n",
        "    bleu_sum = 0.0\n",
        "    bleu_count = 0\n",
        "    for batch in data.iterate_batches(train_data, BATCH_SIZE):\n",
        "      optimiser.zero_grad()\n",
        "      input_seq, out_seq_list, _ , out_idx = model.pack_batch(batch, net.emb, device) #pack every batch: packed input seq, packed output seq, input idx and output idx\n",
        "      enc = net.encode(input_seq) #encode our input seq: outputs the RNNs hidden state of shape(BATCH_SIZE, HIDDEN_STATE_SIZE)\n",
        "      #decode every sequence in our batch individually, getting as a result areference sequence of token IDs & the encoded rep of the in seq created  by the encoder\n",
        "      net_results = []\n",
        "      net_targets = []\n",
        "      for idx, out_seq in enumerate(out_seq_list):\n",
        "        ref_indices = out_idx[idx][1:]\n",
        "        enc_item = net.get_encoded_item(enc, idx)\n",
        "        if random.random() < TEACHER_PROB: #teacher-forcing learning\n",
        "          r = net.decode_teacher(enc_item, out_seq)\n",
        "          bleu_sum += model.seq_bleu(r, ref_indices)\n",
        "        else: #curriculum learning\n",
        "          r, seq = net.decode_chain_argmax(enc_item, out_seq.data[0:1], len(ref_indices))\n",
        "          bleu_sum += utils.calc_bleu(seq, ref_indices)\n",
        "        net_results.append(r)\n",
        "        net_targets.extend(ref_indices)\n",
        "        bleu_count += 1\n",
        "\n",
        "      results_v = torch.cat(net_results)\n",
        "      targets_v = torch.LongTensor(net_targets).to(device)\n",
        "      loss_v = F.cross_entropy(results_v, targets_v)\n",
        "      loss_v.backward()\n",
        "      optimiser.step()\n",
        "      losses.append(loss_v.item())\n",
        "\n",
        "    bleu = bleu_sum / bleu_count\n",
        "    bleu_test = run_test(test_data, net, end_token, device)\n",
        "    log.info(\"Epoch %d: mena loss %.3f, mean BLEU %.3f, test BLEU %.3f\", epoch, np.mean(losses), bleu, bleu_test)\n",
        "    writer.add_scalar(\"loss\", epoch, np.mean(losses))\n",
        "    writer.add_scalar(\"bleu\", epoch, bleu)\n",
        "    writer.add_scalar(\"bleu_test\", epoch, bleu_test)\n",
        "\n",
        "    if best_bleu is None or best_bleu < bleu_test:\n",
        "      if best_bleu is not None:\n",
        "        out_name = os.path.join(saves_path, \"pre_bleu_%.3f_%02.dat\" % (bleu_test, epoch))\n",
        "        torch.save(net.state_dict(), out_name)\n",
        "        log.info(\"Best BLEU updated %.3f\", bleu_test)\n",
        "      best_bleu = bleu_test\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      out_name = os.path.join(saves_path, \"epoch_%03d_%.3f_%.3f_.dat\" % (epoch, bleu, bleu_test))\n",
        "      torch.save(net.state_dict(), out_name)\n",
        "  \n",
        "  writer.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-11 13:04:32,317 INFO Loaded 159 movies with genre comedy\n",
            "2019-12-11 13:04:32,318 INFO Read and tokenise phrases...\n",
            "2019-12-11 13:04:36,722 INFO Loaded 93039 phrases\n",
            "2019-12-11 13:04:37,087 INFO Loaded 24716 dialogues with 93039 phrases, generating training pairs\n",
            "2019-12-11 13:04:37,114 INFO Counting freq of words...\n",
            "2019-12-11 13:04:37,454 INFO Data has 31774 uniq words, 4913 of them occur more than 10\n",
            "2019-12-11 13:04:37,639 INFO Obtained 47644 phrase pairs with 4905 uniq words\n",
            "2019-12-11 13:04:38,029 INFO Training data converted, got 26491 samples\n",
            "2019-12-11 13:04:38,032 INFO Train set has 25166 phrases, test 1325\n",
            "2019-12-11 13:04:41,014 INFO Model: PhraseModel(\n",
            "  (emb): Embedding(4905, 50)\n",
            "  (encoder): LSTM(50, 512, batch_first=True)\n",
            "  (decoder): LSTM(50, 512, batch_first=True)\n",
            "  (output): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4905, bias=True)\n",
            "  )\n",
            ")\n",
            "2019-12-11 13:06:47,880 INFO Epoch 0: mena loss 4.998, mean BLEU 0.156, test BLEU 0.055\n",
            "2019-12-11 13:08:56,545 INFO Epoch 1: mena loss 4.676, mean BLEU 0.169, test BLEU 0.066\n",
            "2019-12-11 13:08:56,569 INFO Best BLEU updated 0.066\n",
            "2019-12-11 13:11:04,845 INFO Epoch 2: mena loss 4.541, mean BLEU 0.180, test BLEU 0.091\n",
            "2019-12-11 13:11:04,871 INFO Best BLEU updated 0.091\n",
            "2019-12-11 13:13:15,586 INFO Epoch 3: mena loss 4.432, mean BLEU 0.184, test BLEU 0.090\n",
            "2019-12-11 13:15:28,266 INFO Epoch 4: mena loss 4.347, mean BLEU 0.188, test BLEU 0.102\n",
            "2019-12-11 13:15:28,290 INFO Best BLEU updated 0.102\n",
            "2019-12-11 13:17:41,358 INFO Epoch 5: mena loss 4.250, mean BLEU 0.193, test BLEU 0.110\n",
            "2019-12-11 13:17:41,384 INFO Best BLEU updated 0.110\n",
            "2019-12-11 13:19:58,455 INFO Epoch 6: mena loss 4.155, mean BLEU 0.198, test BLEU 0.103\n",
            "2019-12-11 13:22:11,609 INFO Epoch 7: mena loss 4.082, mean BLEU 0.203, test BLEU 0.121\n",
            "2019-12-11 13:22:11,636 INFO Best BLEU updated 0.121\n",
            "2019-12-11 13:24:20,065 INFO Epoch 8: mena loss 3.981, mean BLEU 0.211, test BLEU 0.109\n",
            "2019-12-11 13:26:29,419 INFO Epoch 9: mena loss 3.916, mean BLEU 0.217, test BLEU 0.098\n",
            "2019-12-11 13:28:38,932 INFO Epoch 10: mena loss 3.831, mean BLEU 0.226, test BLEU 0.109\n",
            "2019-12-11 13:30:48,241 INFO Epoch 11: mena loss 3.734, mean BLEU 0.235, test BLEU 0.112\n",
            "2019-12-11 13:32:57,403 INFO Epoch 12: mena loss 3.657, mean BLEU 0.247, test BLEU 0.101\n",
            "2019-12-11 13:35:06,298 INFO Epoch 13: mena loss 3.587, mean BLEU 0.258, test BLEU 0.102\n",
            "2019-12-11 13:37:15,264 INFO Epoch 14: mena loss 3.506, mean BLEU 0.272, test BLEU 0.107\n",
            "2019-12-11 13:39:25,094 INFO Epoch 15: mena loss 3.459, mean BLEU 0.280, test BLEU 0.108\n",
            "2019-12-11 13:41:34,702 INFO Epoch 16: mena loss 3.389, mean BLEU 0.292, test BLEU 0.105\n",
            "2019-12-11 13:43:43,726 INFO Epoch 17: mena loss 3.302, mean BLEU 0.306, test BLEU 0.111\n",
            "2019-12-11 13:45:53,644 INFO Epoch 18: mena loss 3.256, mean BLEU 0.317, test BLEU 0.112\n",
            "2019-12-11 13:48:03,416 INFO Epoch 19: mena loss 3.182, mean BLEU 0.333, test BLEU 0.110\n",
            "2019-12-11 13:50:13,844 INFO Epoch 20: mena loss 3.123, mean BLEU 0.344, test BLEU 0.109\n",
            "2019-12-11 13:52:27,921 INFO Epoch 21: mena loss 3.091, mean BLEU 0.356, test BLEU 0.114\n",
            "2019-12-11 13:54:38,900 INFO Epoch 22: mena loss 3.013, mean BLEU 0.371, test BLEU 0.117\n",
            "2019-12-11 13:56:50,110 INFO Epoch 23: mena loss 2.979, mean BLEU 0.380, test BLEU 0.110\n",
            "2019-12-11 13:59:02,125 INFO Epoch 24: mena loss 2.913, mean BLEU 0.397, test BLEU 0.115\n",
            "2019-12-11 14:01:12,454 INFO Epoch 25: mena loss 2.826, mean BLEU 0.412, test BLEU 0.109\n",
            "2019-12-11 14:03:24,726 INFO Epoch 26: mena loss 2.814, mean BLEU 0.421, test BLEU 0.115\n",
            "2019-12-11 14:05:35,867 INFO Epoch 27: mena loss 2.749, mean BLEU 0.432, test BLEU 0.111\n",
            "2019-12-11 14:07:49,249 INFO Epoch 28: mena loss 2.701, mean BLEU 0.445, test BLEU 0.108\n",
            "2019-12-11 14:10:01,525 INFO Epoch 29: mena loss 2.626, mean BLEU 0.463, test BLEU 0.109\n",
            "2019-12-11 14:12:13,409 INFO Epoch 30: mena loss 2.564, mean BLEU 0.476, test BLEU 0.113\n",
            "2019-12-11 14:14:25,739 INFO Epoch 31: mena loss 2.535, mean BLEU 0.488, test BLEU 0.109\n",
            "2019-12-11 14:16:35,411 INFO Epoch 32: mena loss 2.490, mean BLEU 0.495, test BLEU 0.112\n",
            "2019-12-11 14:18:42,474 INFO Epoch 33: mena loss 2.445, mean BLEU 0.506, test BLEU 0.108\n",
            "2019-12-11 14:20:49,737 INFO Epoch 34: mena loss 2.393, mean BLEU 0.519, test BLEU 0.106\n",
            "2019-12-11 14:22:56,000 INFO Epoch 35: mena loss 2.312, mean BLEU 0.537, test BLEU 0.111\n",
            "2019-12-11 14:25:03,709 INFO Epoch 36: mena loss 2.275, mean BLEU 0.549, test BLEU 0.111\n",
            "2019-12-11 14:27:11,006 INFO Epoch 37: mena loss 2.225, mean BLEU 0.561, test BLEU 0.109\n",
            "2019-12-11 14:29:18,833 INFO Epoch 38: mena loss 2.212, mean BLEU 0.564, test BLEU 0.106\n",
            "2019-12-11 14:31:26,115 INFO Epoch 39: mena loss 2.161, mean BLEU 0.574, test BLEU 0.108\n",
            "2019-12-11 14:33:33,594 INFO Epoch 40: mena loss 2.136, mean BLEU 0.579, test BLEU 0.113\n",
            "2019-12-11 14:35:41,115 INFO Epoch 41: mena loss 2.107, mean BLEU 0.588, test BLEU 0.111\n",
            "2019-12-11 14:37:48,290 INFO Epoch 42: mena loss 2.075, mean BLEU 0.597, test BLEU 0.108\n",
            "2019-12-11 14:39:56,491 INFO Epoch 43: mena loss 2.012, mean BLEU 0.609, test BLEU 0.110\n",
            "2019-12-11 14:42:04,702 INFO Epoch 44: mena loss 1.969, mean BLEU 0.621, test BLEU 0.110\n",
            "2019-12-11 14:44:12,865 INFO Epoch 45: mena loss 1.950, mean BLEU 0.625, test BLEU 0.110\n",
            "2019-12-11 14:46:22,729 INFO Epoch 46: mena loss 1.938, mean BLEU 0.629, test BLEU 0.108\n",
            "2019-12-11 14:48:32,465 INFO Epoch 47: mena loss 1.874, mean BLEU 0.643, test BLEU 0.110\n",
            "2019-12-11 14:50:42,521 INFO Epoch 48: mena loss 1.866, mean BLEU 0.647, test BLEU 0.107\n",
            "2019-12-11 14:52:52,889 INFO Epoch 49: mena loss 1.788, mean BLEU 0.658, test BLEU 0.109\n",
            "2019-12-11 14:55:04,956 INFO Epoch 50: mena loss 1.789, mean BLEU 0.663, test BLEU 0.109\n",
            "2019-12-11 14:57:15,253 INFO Epoch 51: mena loss 1.769, mean BLEU 0.663, test BLEU 0.107\n",
            "2019-12-11 14:59:26,247 INFO Epoch 52: mena loss 1.801, mean BLEU 0.663, test BLEU 0.107\n",
            "2019-12-11 15:01:37,444 INFO Epoch 53: mena loss 1.760, mean BLEU 0.667, test BLEU 0.104\n",
            "2019-12-11 15:03:49,008 INFO Epoch 54: mena loss 1.749, mean BLEU 0.669, test BLEU 0.103\n",
            "2019-12-11 15:05:58,048 INFO Epoch 55: mena loss 1.763, mean BLEU 0.664, test BLEU 0.104\n",
            "2019-12-11 15:08:07,743 INFO Epoch 56: mena loss 1.747, mean BLEU 0.667, test BLEU 0.107\n",
            "2019-12-11 15:10:18,813 INFO Epoch 57: mena loss 1.688, mean BLEU 0.680, test BLEU 0.107\n",
            "2019-12-11 15:12:26,534 INFO Epoch 58: mena loss 1.668, mean BLEU 0.683, test BLEU 0.104\n",
            "2019-12-11 15:14:34,773 INFO Epoch 59: mena loss 1.662, mean BLEU 0.685, test BLEU 0.108\n",
            "2019-12-11 15:16:44,973 INFO Epoch 60: mena loss 1.647, mean BLEU 0.691, test BLEU 0.106\n",
            "2019-12-11 15:18:53,613 INFO Epoch 61: mena loss 1.607, mean BLEU 0.696, test BLEU 0.103\n",
            "2019-12-11 15:21:00,811 INFO Epoch 62: mena loss 1.573, mean BLEU 0.702, test BLEU 0.102\n",
            "2019-12-11 15:23:09,643 INFO Epoch 63: mena loss 1.559, mean BLEU 0.709, test BLEU 0.108\n",
            "2019-12-11 15:25:18,876 INFO Epoch 64: mena loss 1.585, mean BLEU 0.707, test BLEU 0.111\n",
            "2019-12-11 15:27:26,365 INFO Epoch 65: mena loss 1.535, mean BLEU 0.716, test BLEU 0.107\n",
            "2019-12-11 15:29:33,988 INFO Epoch 66: mena loss 1.500, mean BLEU 0.720, test BLEU 0.103\n",
            "2019-12-11 15:31:41,330 INFO Epoch 67: mena loss 1.429, mean BLEU 0.736, test BLEU 0.105\n",
            "2019-12-11 15:33:48,790 INFO Epoch 68: mena loss 1.398, mean BLEU 0.743, test BLEU 0.103\n",
            "2019-12-11 15:35:57,537 INFO Epoch 69: mena loss 1.407, mean BLEU 0.745, test BLEU 0.103\n",
            "2019-12-11 15:38:05,537 INFO Epoch 70: mena loss 1.389, mean BLEU 0.746, test BLEU 0.105\n",
            "2019-12-11 15:40:14,401 INFO Epoch 71: mena loss 1.410, mean BLEU 0.744, test BLEU 0.105\n",
            "2019-12-11 15:42:24,546 INFO Epoch 72: mena loss 1.424, mean BLEU 0.739, test BLEU 0.108\n",
            "2019-12-11 15:44:33,090 INFO Epoch 73: mena loss 1.360, mean BLEU 0.752, test BLEU 0.107\n",
            "2019-12-11 15:46:41,764 INFO Epoch 74: mena loss 1.359, mean BLEU 0.753, test BLEU 0.108\n",
            "2019-12-11 15:48:50,936 INFO Epoch 75: mena loss 1.335, mean BLEU 0.760, test BLEU 0.106\n",
            "2019-12-11 15:51:00,927 INFO Epoch 76: mena loss 1.326, mean BLEU 0.759, test BLEU 0.106\n",
            "2019-12-11 15:53:08,503 INFO Epoch 77: mena loss 1.308, mean BLEU 0.764, test BLEU 0.102\n",
            "2019-12-11 15:55:17,897 INFO Epoch 78: mena loss 1.284, mean BLEU 0.768, test BLEU 0.106\n",
            "2019-12-11 15:57:24,194 INFO Epoch 79: mena loss 1.270, mean BLEU 0.771, test BLEU 0.107\n",
            "2019-12-11 15:59:31,135 INFO Epoch 80: mena loss 1.258, mean BLEU 0.773, test BLEU 0.107\n",
            "2019-12-11 16:01:40,224 INFO Epoch 81: mena loss 1.276, mean BLEU 0.770, test BLEU 0.106\n",
            "2019-12-11 16:03:50,609 INFO Epoch 82: mena loss 1.313, mean BLEU 0.763, test BLEU 0.108\n",
            "2019-12-11 16:06:00,506 INFO Epoch 83: mena loss 1.318, mean BLEU 0.757, test BLEU 0.101\n",
            "2019-12-11 16:08:08,374 INFO Epoch 84: mena loss 1.314, mean BLEU 0.756, test BLEU 0.107\n",
            "2019-12-11 16:10:16,943 INFO Epoch 85: mena loss 1.313, mean BLEU 0.762, test BLEU 0.106\n",
            "2019-12-11 16:12:27,727 INFO Epoch 86: mena loss 1.341, mean BLEU 0.755, test BLEU 0.102\n",
            "2019-12-11 16:14:35,055 INFO Epoch 87: mena loss 1.286, mean BLEU 0.764, test BLEU 0.102\n",
            "2019-12-11 16:16:42,522 INFO Epoch 88: mena loss 1.247, mean BLEU 0.771, test BLEU 0.105\n",
            "2019-12-11 16:18:52,349 INFO Epoch 89: mena loss 1.250, mean BLEU 0.771, test BLEU 0.105\n",
            "2019-12-11 16:21:01,606 INFO Epoch 90: mena loss 1.272, mean BLEU 0.767, test BLEU 0.102\n",
            "2019-12-11 16:23:09,681 INFO Epoch 91: mena loss 1.306, mean BLEU 0.760, test BLEU 0.106\n",
            "2019-12-11 16:25:18,752 INFO Epoch 92: mena loss 1.283, mean BLEU 0.764, test BLEU 0.105\n",
            "2019-12-11 16:27:26,795 INFO Epoch 93: mena loss 1.235, mean BLEU 0.774, test BLEU 0.103\n",
            "2019-12-11 16:29:34,814 INFO Epoch 94: mena loss 1.206, mean BLEU 0.781, test BLEU 0.107\n",
            "2019-12-11 16:31:43,999 INFO Epoch 95: mena loss 1.165, mean BLEU 0.790, test BLEU 0.107\n",
            "2019-12-11 16:33:54,318 INFO Epoch 96: mena loss 1.163, mean BLEU 0.792, test BLEU 0.103\n",
            "2019-12-11 16:36:04,050 INFO Epoch 97: mena loss 1.169, mean BLEU 0.790, test BLEU 0.107\n",
            "2019-12-11 16:38:11,301 INFO Epoch 98: mena loss 1.183, mean BLEU 0.782, test BLEU 0.104\n",
            "2019-12-11 16:40:20,503 INFO Epoch 99: mena loss 1.192, mean BLEU 0.781, test BLEU 0.103\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW0hLnfe15g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}